import os
import logging
import requests
import json
from typing import TypedDict
from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


from dotenv import load_dotenv
load_dotenv()

# –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
os.makedirs("logs", exist_ok=True)
logger = logging.getLogger("agent_logger") 
logger.setLevel(logging.INFO)

info_handler = logging.FileHandler("logs/agent.log", encoding="utf-8") # –æ–±—â–∏–π –ª–æ–≥-—Ñ–∞–π–ª
info_handler.setLevel(logging.INFO)

error_handler = logging.FileHandler("logs/errors.log", encoding="utf-8") # –ª–æ–≥-—Ñ–∞–π–ª –¥–ª—è –æ—à–∏–±–æ–∫
error_handler.setLevel(logging.ERROR)

formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
info_handler.setFormatter(formatter)
error_handler.setFormatter(formatter)

logger.addHandler(info_handler)
logger.addHandler(error_handler)

class State(TypedDict):
    text: str
    classification: str
    sentiment: str
    entities: dict
    summary: str
    knowledge: str
    response: str

llm = ChatOpenAI(
    model="gpt-4.1-nano",
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE"),
    temperature=0
)

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)

def pre_filter_node(state: State):
    text = state["text"].lower()
    if len(text) < 10 or any(p in text for p in ["—Å–ø–∞—Å–∏–±–æ", "–æ–∫", "–∞–≥–∞"]):
        return {"response": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å ‚Äî —Å–µ–π—á–∞—Å –æ–Ω —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π."}
    if not any(x in text for x in ["–∑–∞–∫–∞–∑", "—Ç–æ–≤–∞—Ä", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–ø–ª–∞—Ç–∞", "–ø—Ä–æ–º–æ–∫–æ–¥", "–≤–æ–∑–≤—Ä–∞—Ç", "–∂–∞–ª–æ–±–∞", 
                                   "–æ—Ç–∑—ã–≤", "–≥–∞—Ä–∞–Ω—Ç–∏—è", "–∞–¥—Ä–µ—Å", "–æ—Ñ–æ—Ä–º–∏—Ç—å", "—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç", "—Å—Ç–∞—Ç—É—Å"]):
        return {"response": "–ü–æ—Ö–æ–∂–µ, –≤–∞—à –∑–∞–ø—Ä–æ—Å –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º —Ç–µ–º–∞–º. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ."}
    return {}

def classify_node(state: State):
    prompt = PromptTemplate(
        input_variables=["text"],
        template="Classify the following customer message into a category (Order, Return, Complaint, Other):\n\n{text}\n\nCategory:"
    )
    message = HumanMessage(content=prompt.format(text=state["text"]))
    classification = llm.invoke([message]).content.strip()
    return {"classification": classification}

def sentiment_node(state: State):
    prompt = PromptTemplate(
        input_variables=["text"],
        template="What is the sentiment of the following message? (Positive, Neutral, Negative)\n\n{text}\n\nSentiment:"
    )
    message = HumanMessage(content=prompt.format(text=state["text"]))
    sentiment = llm.invoke([message]).content.strip()
    return {"sentiment": sentiment}

def entity_node(state: State):
    prompt = PromptTemplate(
        input_variables=["text"],
        template="Extract all named entities (User ID, User Name, Order ID, Product Category, " \
        "Order Date, Address, Email, Phone Number, Delivery Method, Payment Method, Promocode) " \
        "from the text:\n\n{text}\n\nEntities (comma-separated):"
    )
    message = HumanMessage(content=prompt.format(text=state["text"]))
    # entities = llm.invoke([message]).content.strip().split(", ")
    # return {"entities": entities}
    # import json
    # try:
    #     entities = json.loads(llm.invoke([message]).content.strip())
    # except json.JSONDecodeError:
    #     entities = {}
    # return {"entities": entities}
    raw_output = llm.invoke([message]).content.strip()
    entity_dict = {}
    for line in raw_output.split("\n"):
        if ":" in line:
            key, value = line.split(":", 1)
            entity_dict[key.strip()] = value.strip()
    
    return {"entities": entity_dict}


def summary_node(state: State):
    prompt = PromptTemplate(
        input_variables=["text"],
        template="Summarize the customer's message in one sentence in Russian:\n\n{text}\n\nSummary:"
    )
    message = HumanMessage(content=prompt.format(text=state["text"]))
    summary = llm.invoke([message]).content.strip()
    return {"summary": summary}

def retrieve_knowledge_node(state: State):
    query = state["text"]
    docs = vectorstore.similarity_search_with_score(query, k=3)
    for doc, score in docs:
        logger.info(f"Query: {query} | Score: {score:.4f} | Content: {doc.page_content[:200]}...")
    relevant_docs = [doc for doc, score in docs if score < 0.7]  # –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ—Ä–æ–≥
    if not relevant_docs:
        return {"knowledge": ""}
    context = "\n".join([doc.page_content for doc in relevant_docs])
    return {"knowledge": context}

def response_node(state: State):
    if not all(k in state for k in ("classification", "sentiment", "knowledge")):
        return {"response": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –µ–≥–æ."}
    
    prompt = PromptTemplate(
        input_variables=["text", "classification", "sentiment", "knowledge"],
        template=(
            "You are a helpful support agent. Use the following context to respond:\n\n"
            "Context:\n{knowledge}\n\n"
            "Message: {text}\nCategory: {classification}\nSentiment: {sentiment}\n\nResponse:"
        )
    )
    message = HumanMessage(content=prompt.format(
        text=state["text"],
        classification=state["classification"],
        sentiment=state["sentiment"],
        knowledge=state["knowledge"]
    ))
    
    try:
        response = llm.invoke([message]).content.strip()
    except Exception as e:
        logger.error(f"LLM error: {str(e)} | Input: {state}")
        response = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –ø–æ–∑–∂–µ."

    logger.info(json.dumps({
        "user_message": state["text"],
        "category": state["classification"],
        "sentiment": state["sentiment"],
        "entities": state.get("entities", []),
        "summary": state.get("summary", ""),
        "knowledge": state["knowledge"],
        "response": response
    }, ensure_ascii=False, indent=2))

    return {"response": response}

def send_telegram_alert(text: str):
    token = os.getenv("TELEGRAM_BOT_TOKEN")
    chat_id = os.getenv("TELEGRAM_CHAT_ID")
    if not token or not chat_id:
        logger.warning("Telegram token or chat_id not set.")
        return

    message = f"üö® –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∞–ª–æ–±—ã –∫–ª–∏–µ–Ω—Ç–∞:\n\n{text}"
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {"chat_id": chat_id, "text": message}
    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {str(e)}")

def escalate_node(state: State):
    message = "–í–∞—à –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–¥–∞–Ω –æ–ø–µ—Ä–∞—Ç–æ—Ä—É –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è."
    logger.info(f"Input: {state['text']} | Escalation triggered")
    send_telegram_alert(state["summary"])
    return {"response": message}

def review_request_node(state: State):
    message = "–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤! –•–æ—Ç–∏—Ç–µ –æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–∑—ã–≤ –Ω–∞ —Å–∞–π—Ç–µ –æ —Ç–æ–≤–∞—Ä–µ?"
    logger.info(f"Input: {state['text']} | Review requested")
    return {"response": message}

def route_request(state: State) -> str:
    if not state["knowledge"]:
        return "no_knowledge_or_negative"
    if state["classification"] == "Complaint" and state["sentiment"] == "Negative":
        return "no_knowledge_or_negative"
    if state["classification"] == "Feedback" and state["sentiment"] == "Positive":
        return "positive_feedback"
    else:
        return "respond"

workflow = StateGraph(State)
workflow.add_node("pre_filter", pre_filter_node)
workflow.add_node("classify", classify_node)
workflow.add_node("analyze_sentiment", sentiment_node)
workflow.add_node("entity", entity_node)
workflow.add_node("summary_message", summary_node)
workflow.add_node("retrieve_knowledge", retrieve_knowledge_node)
workflow.add_node("get_response", response_node)
workflow.add_node("escalate_to_operator", escalate_node)
workflow.add_node("ask_for_review", review_request_node)

workflow.set_entry_point("pre_filter")
workflow.add_conditional_edges(
    "pre_filter",
    lambda state: "get_response" if "response" in state else "classify",
    {
        "get_response": "get_response",
        "classify": "classify"
    }
)
workflow.add_edge("classify", "analyze_sentiment")
workflow.add_edge("analyze_sentiment", "entity")
workflow.add_edge("entity", "summary_message")
workflow.add_edge("summary_message", "retrieve_knowledge")
workflow.add_conditional_edges(
    "retrieve_knowledge",
    route_request,
    {
        "respond": "get_response",
        "no_knowledge_or_negative": "escalate_to_operator",
        "positive_feedback": "ask_for_review"
    }
)
workflow.add_edge("get_response", END)
workflow.add_edge("escalate_to_operator", END)
workflow.add_edge("ask_for_review", END)

agent = workflow.compile()
