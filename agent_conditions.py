import os
import logging
import requests
import json
import traceback
import uuid
from typing import TypedDict
from langgraph.graph import StateGraph, END
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory 

from dotenv import load_dotenv
load_dotenv()

# –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
os.makedirs("logs", exist_ok=True)
logger = logging.getLogger("agent_logger") 
logger.setLevel(logging.INFO)

info_handler = logging.FileHandler("logs/agent.log", encoding="utf-8") # –æ–±—â–∏–π –ª–æ–≥-—Ñ–∞–π–ª
info_handler.setLevel(logging.INFO)

error_handler = logging.FileHandler("logs/errors.log", encoding="utf-8") # –ª–æ–≥-—Ñ–∞–π–ª –¥–ª—è –æ—à–∏–±–æ–∫
error_handler.setLevel(logging.ERROR)

formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
info_handler.setFormatter(formatter)
error_handler.setFormatter(formatter)

logger.addHandler(info_handler)
logger.addHandler(error_handler)

class State(TypedDict):
    text: str
    classification: str
    sentiment: str
    entities: dict
    summary: str
    knowledge: str
    response: str
    memory: str

llm = ChatOpenAI(
    model="gpt-4.1-nano",
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE"),
    temperature=0
)

# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)

chat_memory = ConversationBufferMemory(
    llm=llm,
    memory_key="history",
    return_messages=True,
    max_token_limit=1000
)

def run_agent(user_input: str, verbose: bool = False) -> str:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç.
    """
    state = {"text": user_input}
    try:
        for step in agent.stream(state):
            if verbose:
                print(f"Step: {step}")
                print(f"Data: {agent.get_state(step)}\n")
        final = agent.invoke(state)
        if verbose:
            print(f"Final state: {final}\n")
        return final["response"]
    except Exception as e:
        error_id = str(uuid.uuid4())[:8]  # –∫–æ—Ä–æ—Ç–∫–∏–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –æ—à–∏–±–∫–∏
        error_type = type(e).__name__
        trace = traceback.format_exc()
        logger.error(f"[{error_type}] Error ID: {error_id} | Input: {user_input}\n{str(e)}\nTraceback:\n{trace}")
        return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ (ID: {error_id}). –ú—ã —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ –µ—ë —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º."

def pre_filter_node(state: State):
    """
    –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π.
    """
    text = state["text"].lower()
    if len(text) < 10 or any(p in text for p in ["—Å–ø–∞—Å–∏–±–æ", "–æ–∫", "–∞–≥–∞"]):
        return {"response": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å ‚Äî —Å–µ–π—á–∞—Å –æ–Ω —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π."}
    if not any(x in text for x in ["–∑–∞–∫–∞–∑", "—Ç–æ–≤–∞—Ä", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–ø–ª–∞—Ç–∞", "–ø—Ä–æ–º–æ–∫–æ–¥", "–≤–æ–∑–≤—Ä–∞—Ç", "–∂–∞–ª–æ–±–∞", 
                                   "–æ—Ç–∑—ã–≤", "–≥–∞—Ä–∞–Ω—Ç–∏—è", "–∞–¥—Ä–µ—Å", "–æ—Ñ–æ—Ä–º–∏—Ç—å", "—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç", "—Å—Ç–∞—Ç—É—Å"]):
        return {"response": "–ü–æ—Ö–æ–∂–µ, –≤–∞—à –∑–∞–ø—Ä–æ—Å –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º —Ç–µ–º–∞–º. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ."}
    return {}

def classify_node(state: State):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: Order, Return, Complaint, Feedback, Other.
    """
    prompt = PromptTemplate(
        input_variables=["text"],
        template="Classify the following customer message into a category (Order, Return, Complaint, Feedback, Other):\n\n{text}\n\nCategory:"
    )
    message = HumanMessage(content=prompt.format(text=state["text"]))
    classification = llm.invoke([message]).content.strip()

    valid_categories = {"Order", "Return", "Complaint", "Feedback", "Other"}
    if classification not in valid_categories:
        classification = "Other"

    return {"classification": classification}

def sentiment_node(state: State):
    """
    –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: Positive, Neutral, Negative.
    """
    prompt = PromptTemplate(
        input_variables=["text"],
        template="What is the sentiment of the following message? (Positive, Neutral, Negative)\n\n{text}\n\nSentiment:"
    )
    message = HumanMessage(content=prompt.format(text=state["text"]))
    sentiment = llm.invoke([message]).content.strip()
    return {"sentiment": sentiment}

def entity_node(state: State):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, ID –∑–∞–∫–∞–∑–∞, –∞–¥—Ä–µ—Å, email, etc.).
    """
    prompt = PromptTemplate(
        input_variables=["text"],
        template="Extract all named entities (User ID, User Name, Order ID, Product Category, " \
        "Order Date, Address, Email, Phone Number, Delivery Method, Payment Method, Promocode) " \
        "from the text:\n\n{text}\n\nEntities (comma-separated):"
    )
    message = HumanMessage(content=prompt.format(text=state["text"]))
    raw_output = llm.invoke([message]).content.strip()
    entity_dict = {}
    for line in raw_output.split("\n"):
        if ":" in line:
            key, value = line.split(":", 1)
            entity_dict[key.strip()] = value.strip()
    
    return {"entities": entity_dict}

def summary_node(state: State):
    """
    –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
    """
    prompt = PromptTemplate(
        input_variables=["text"],
        template="Summarize the customer's message in one sentence in Russian:\n\n{text}\n\nSummary:"
    )
    message = HumanMessage(content=prompt.format(text=state["text"]))
    summary = llm.invoke([message]).content.strip()
    return {"summary": summary}

def retrieve_knowledge_node(state: State):
    """
    –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.
    """
    query = state["text"]
    docs = vectorstore.similarity_search_with_score(query, k=3)
    for doc, score in docs:
        logger.info(f"Query: {query} | Score: {score:.4f} | Content: {doc.page_content[:200]}...")
    relevant_docs = [doc for doc, score in docs if score < 0.7]  # –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ—Ä–æ–≥
    if not relevant_docs:
        return {"knowledge": ""}
    context = "\n".join([doc.page_content for doc in relevant_docs])
    return {"knowledge": context}

def response_node(state: State):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å —É—á–µ—Ç–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, –ø–∞–º—è—Ç–∏ –∏ –∑–Ω–∞–Ω–∏–π.
    """
    memory_summary = chat_memory.load_memory_variables({}).get("history", "")
    
    prompt = PromptTemplate(
        input_variables=["text", "classification", "sentiment", "knowledge", "history"],
        template=(
            "You are a helpful support agent. Use the following context to respond:\n\n"
            "History:\n{history}\n\n"
            "Context:\n{knowledge}\n\n"
            "Message: {text}\nCategory: {classification}\nSentiment: {sentiment}\n\nResponse:"
        )
    )
    message = HumanMessage(content=prompt.format(
        text=state["text"],
        classification=state.get("classification", "Other"),
        sentiment=state.get("sentiment", "Neutral"),
        knowledge=state.get("knowledge", ""),
        history=memory_summary
    ))
    
    try:
        response = llm.invoke([message]).content.strip()
        chat_memory.save_context({"input": state["text"]}, {"output": response})
    except Exception as e:
        error_type = type(e).__name__
        trace = traceback.format_exc()
        logger.error(f"[{error_type}] LLM error in response_node: {str(e)} | Input: {state}\nTraceback:\n{trace}")
        response = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –ø–æ–∑–∂–µ."

    logger.info(json.dumps({
        "user_message": state["text"],
        "category": state.get("classification", "Unknown"),
        "sentiment": state.get("sentiment", "Unknown"),
        "entities": state.get("entities", {}),
        "summary": state.get("summary", ""),
        "knowledge": state.get("knowledge", ""),
        "response": response
    }, ensure_ascii=False, indent=2))


    return {"response": response}

def send_telegram_alert(text: str):
    """
    –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∞–º–º–∞—Ä–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É –≤ Telegram.
    """
    token = os.getenv("TELEGRAM_BOT_TOKEN")
    chat_id = os.getenv("TELEGRAM_CHAT_ID")
    if not token or not chat_id:
        logger.warning("Telegram token or chat_id not set.")
        return

    message = f"üö® –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∞–ª–æ–±—ã –∫–ª–∏–µ–Ω—Ç–∞:\n\n{text}"

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {"chat_id": chat_id, "text": message}
    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {str(e)}")

def escalate_node(state: State):
    """
    –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –∂–∞–ª–æ–±—ã –æ–ø–µ—Ä–∞—Ç–æ—Ä—É –≤ Telegram.
    """
    message = "–í–∞—à –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–¥–∞–Ω –æ–ø–µ—Ä–∞—Ç–æ—Ä—É –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è."
    logger.info(f"Input: {state['text']} | Escalation triggered")
    send_telegram_alert(state["summary"])
    return {"response": message}

def review_request_node(state: State):
    """
    –ü—Ä–æ—Å—å–±–∞ –æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–∑—ã–≤, –µ—Å–ª–∏ –æ—Ç–∑—ã–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π.
    """
    message = "–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤! –•–æ—Ç–∏—Ç–µ –æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç–∑—ã–≤ –Ω–∞ —Å–∞–π—Ç–µ –æ —Ç–æ–≤–∞—Ä–µ?"
    logger.info(f"Input: {state['text']} | Review requested")
    return {"response": message}

def route_request(state: State) -> str:
    """
    –õ–æ–≥–∏–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –≤ –≥—Ä–∞—Ñ–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–ª–∏—á–∏—è –∑–Ω–∞–Ω–∏–π.
    """
    if state["classification"] == "Feedback" and state["sentiment"] == "Positive":
        return "positive_feedback"
    if not state["knowledge"]:
        return "no_knowledge_or_negative"
    if state["classification"] == "Complaint" and state["sentiment"] == "Negative":
        return "no_knowledge_or_negative"
    return "respond"

# –ì—Ä–∞—Ñ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
workflow = StateGraph(State)
workflow.add_node("pre_filter", pre_filter_node)
workflow.add_node("classify", classify_node)
workflow.add_node("analyze_sentiment", sentiment_node)
workflow.add_node("entity", entity_node)
workflow.add_node("summary_message", summary_node)
workflow.add_node("retrieve_knowledge", retrieve_knowledge_node)
workflow.add_node("get_response", response_node)
workflow.add_node("escalate_to_operator", escalate_node)
workflow.add_node("ask_for_review", review_request_node)

workflow.set_entry_point("pre_filter")
workflow.add_conditional_edges(
    "pre_filter",
    lambda state: "get_response" if "response" in state else "classify",
    {
        "get_response": "get_response",
        "classify": "classify"
    }
)
workflow.add_edge("classify", "analyze_sentiment")
workflow.add_edge("analyze_sentiment", "entity")
workflow.add_edge("entity", "summary_message")
workflow.add_edge("summary_message", "retrieve_knowledge")
workflow.add_conditional_edges(
    "retrieve_knowledge",
    route_request,
    {
        "respond": "get_response",
        "no_knowledge_or_negative": "escalate_to_operator",
        "positive_feedback": "ask_for_review"
    }
)
workflow.add_edge("get_response", END)
workflow.add_edge("escalate_to_operator", END)
workflow.add_edge("ask_for_review", END)

agent = workflow.compile()
